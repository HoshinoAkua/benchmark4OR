In operations research (OR), formulating optimization problems in industrial applications is often time-consuming and requires specialized expertise. Recently, large language models (LLMs) have shown remarkable potential to automate this process. However, evaluating the performance of LLMs in optimization modeling remains challenging due to the scarcity of suitable datasets and rigorous evaluation methodologies. To reduce this gap, we introduce OptiBench, a new benchmark designed to assess LLMs' ability to formulate linear programming (LP) and mixed-integer linear programming (MILP) models. OptiBench provides a diverse dataset of optimization word problems, adopting a model-data separation format inspired by INFORMS modeling competitions. This design closely mirrors the complexity of real-world optimization problems compared to traditional textbook examples. Moreover, OptiBench incorporates a new evaluation method based on a modified Weisfeiler-Lehman graph isomorphism test (WL-test) algorithm. We theoretically prove that this method accurately assesses the equivalence of models generated from our data, setting a new standard for automatically validating the correctness of optimization modeling. We benchmark various LLMs using OptiBench and observe significant performance differences. GPT-4o by direct prompting achieves 52.25\% overall accuracy, outperforming other models and LLM-based agents, including OpenAI o1 (preview and mini). Notably, GPT-4o's performance varies across different problem classes, achieving over 90\% accuracy on the knapsack problem class but falling below 5\% on the traveling salesman problem class. These findings provide new insights into the strengths and limitations of LLMs in optimization modeling. 